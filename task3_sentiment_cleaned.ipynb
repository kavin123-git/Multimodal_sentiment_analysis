{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "zip_path = \"/content/drive/MyDrive/images.zip\"  # Update with correct file name\n",
        "extract_path = \"/content/images\"  # Destination folder\n",
        "\n",
        "# Create extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction Complete! Files are in:\", extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers timm torch torchvision pandas pillow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def clean_dataset(csv_file_path, img_dir_path):\n",
        "    \"\"\"\n",
        "    Cleans the dataset by removing images that cannot be read or have empty captions.\n",
        "\n",
        "    Args:\n",
        "        csv_file_path (str): Path to the CSV file containing image labels.\n",
        "        img_dir_path (str): Path to the directory containing the images.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Filter out images with issues:\n",
        "    indices_to_remove = []\n",
        "    for index, row in df.iterrows():\n",
        "        img_name = os.path.join(img_dir_path, str(row['image_name']))  # Assuming 'image_name' column\n",
        "        caption = row['text_corrected']  # Assuming 'text_corrected' column\n",
        "\n",
        "        # Check for empty caption:\n",
        "        if not isinstance(caption, str) or not caption.strip():\n",
        "            print(f\"Removing entry with empty or invalid caption: {img_name}\")\n",
        "            indices_to_remove.append(index)\n",
        "            continue\n",
        "\n",
        "        # Check if image can be read:\n",
        "        try:\n",
        "            Image.open(img_name).convert(\"RGB\")\n",
        "        except (FileNotFoundError, IOError, OSError):\n",
        "            print(f\"Removing entry with unreadable image: {img_name}\")\n",
        "            indices_to_remove.append(index)\n",
        "            # Optional: If you want to delete the image file itself:\n",
        "            # os.remove(img_name)\n",
        "\n",
        "    # Remove problematic entries from DataFrame:\n",
        "    cleaned_df = df.drop(indices_to_remove)\n",
        "\n",
        "    # Overwrite original CSV file with cleaned data:\n",
        "    cleaned_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "    print(f\"Removed {len(indices_to_remove)} entries from the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change these paths to match your dataset location\n",
        "CSV_FILE_PATH = \"/content/labels.csv\"  # Replace with actual path\n",
        "IMG_DIR_PATH = \"/content/images\"  # Replace with actual path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image Preprocessing\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Pretrained Tokenizer (BERT/RoBERTa)\n",
        "TEXT_MODEL = \"bert-base-uncased\"  # Change to \"roberta-base\" if using RoBERTa\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, tokenizer, transform=None, max_length=128):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Label Mapping\n",
        "        self.label_mapping = {\n",
        "            \"not_funny\": 0, \"funny\": 1, \"very_funny\": 2, \"hilarious\": 3,\n",
        "            \"not_sarcastic\": 0, \"general\": 1, \"twisted_meaning\": 2, \"very_twisted\": 3,\n",
        "            \"not_offensive\": 0, \"slight\": 1, \"very_offensive\": 2, \"hateful_offensive\": 3,\n",
        "            \"not_motivational\": 0, \"motivational\": 1\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load Image\n",
        "            img_name = os.path.join(self.img_dir, str(self.data.iloc[idx, 0]))\n",
        "            image = Image.open(img_name).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # Process Text\n",
        "            caption = self.data.iloc[idx, 2]  # \"text_corrected\"\n",
        "            if not isinstance(caption, str):\n",
        "                caption = str(caption)  # Convert to string if necessary\n",
        "\n",
        "            # Check if caption is empty and skip if it is\n",
        "            if not caption.strip():\n",
        "                print(f\"Skipping empty caption for image: {img_name}\")\n",
        "                raise FileNotFoundError  # Reuse FileNotFoundError for consistency\n",
        "\n",
        "            encoding = self.tokenizer(caption, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "            text_input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "            text_attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "            # Map Labels\n",
        "            humor = self.label_mapping.get(str(self.data.iloc[idx, 3]), -1)  # Default -1 for errors\n",
        "            sarcasm = self.label_mapping.get(str(self.data.iloc[idx, 4]), -1)\n",
        "            offense = self.label_mapping.get(str(self.data.iloc[idx, 5]), -1)\n",
        "            motivation = self.label_mapping.get(str(self.data.iloc[idx, 6]), -1)\n",
        "\n",
        "            labels = torch.tensor([humor, sarcasm, offense, motivation], dtype=torch.float32)\n",
        "\n",
        "            return text_input_ids, text_attention_mask, image, labels\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found or empty caption: {img_name}, skipping...\")\n",
        "            # Instead of returning empty tensors, return tensors with valid token IDs\n",
        "            return torch.zeros(self.max_length, dtype=torch.long), torch.zeros(self.max_length, dtype=torch.long), torch.zeros(3, 224, 224, dtype=torch.float32), torch.tensor([-1, -1, -1, -1], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset & DataLoader\n",
        "clean_dataset(CSV_FILE_PATH, IMG_DIR_PATH)\n",
        "dataset = MemeDataset(CSV_FILE_PATH, IMG_DIR_PATH, tokenizer, transform=image_transform)\n",
        "# Check a sample batch\n",
        "for text_input_ids, text_attention_mask, images, labels in dataloader:\n",
        "    print(\"Text input shape:\", text_input_ids.shape)  # (batch_size, max_length)\n",
        "    print(\"Image shape:\", images.shape)  # (batch_size, 3, 224, 224)\n",
        "    print(\"Labels shape:\", labels.shape)  # (batch_size, 4)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Pretrained Text Transformer (BERT or RoBERTa)\n",
        "text_encoder = AutoModel.from_pretrained(TEXT_MODEL)\n",
        "\n",
        "# Load Pretrained Image Model (ResNet50 or ViT)\n",
        "IMAGE_MODEL = \"resnet50\"  # Change to \"vit_base_patch16_224\" for ViT\n",
        "if IMAGE_MODEL == \"resnet50\":\n",
        "    image_encoder = models.resnet50(pretrained=True)\n",
        "    image_encoder.fc = nn.Identity()  # Remove classification head\n",
        "elif IMAGE_MODEL == \"vit_base_patch16_224\":\n",
        "    import timm\n",
        "    image_encoder = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "    image_encoder.head = nn.Identity()  # Remove classification head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, text_model, image_model, fusion_dim=512):\n",
        "        super(MultimodalModel, self).__init__()\n",
        "        self.text_encoder = text_model\n",
        "        self.image_encoder = image_model\n",
        "\n",
        "        text_embedding_dim = 768\n",
        "        image_embedding_dim = 2048 if IMAGE_MODEL == \"resnet50\" else 768\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Linear(text_embedding_dim + image_embedding_dim, fusion_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Regression head (Predicts Humor, Sarcasm, Offense, Motivation)\n",
        "        self.regression_head = nn.Linear(fusion_dim, 4)\n",
        "\n",
        "    def forward(self, text_input_ids, text_attention_mask, image_tensor):\n",
        "        # Text encoding\n",
        "        text_features = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask).last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Image encoding\n",
        "        image_features = self.image_encoder(image_tensor).squeeze()\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = torch.cat((text_features, image_features), dim=1)\n",
        "        fused_output = self.relu(self.fusion(fused_features))\n",
        "\n",
        "        # Regression head\n",
        "        output = self.regression_head(fused_output)  # Outputs 4 values\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalModel(text_encoder, image_encoder).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for text_input_ids, text_attention_mask, images, labels in dataloader:\n",
        "        text_input_ids, text_attention_mask, images, labels = (\n",
        "            text_input_ids.to(device), text_attention_mask.to(device), images.to(device), labels.to(device)\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(text_input_ids, text_attention_mask, images)\n",
        "        loss = criterion(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Store predictions & true labels for metric calculation\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_preds = np.vstack(all_preds)  # Shape: (num_samples, 4)\n",
        "    all_labels = np.vstack(all_labels)  # Shape: (num_samples, 4)\n",
        "\n",
        "    # Convert Regression Outputs to Discrete Labels (Round to Nearest Integer)\n",
        "    all_preds = np.round(all_preds).astype(int)\n",
        "\n",
        "    # Compute Metrics for Each Category\n",
        "    f1_humor = f1_score(all_labels[:, 0], all_preds[:, 0], average=\"macro\")\n",
        "    f1_sarcasm = f1_score(all_labels[:, 1], all_preds[:, 1], average=\"macro\")\n",
        "    f1_offense = f1_score(all_labels[:, 2], all_preds[:, 2], average=\"macro\")\n",
        "    f1_motivation = f1_score(all_labels[:, 3], all_preds[:, 3], average=\"macro\")\n",
        "\n",
        "    acc_humor = accuracy_score(all_labels[:, 0], all_preds[:, 0])\n",
        "    acc_sarcasm = accuracy_score(all_labels[:, 1], all_preds[:, 1])\n",
        "    acc_offense = accuracy_score(all_labels[:, 2], all_preds[:, 2])\n",
        "    acc_motivation = accuracy_score(all_labels[:, 3], all_preds[:, 3])\n",
        "\n",
        "    avg_f1 = (f1_humor + f1_sarcasm + f1_offense + f1_motivation) / 4\n",
        "    avg_acc = (acc_humor + acc_sarcasm + acc_offense + acc_motivation) / 4\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss / len(dataloader):.4f}\")\n",
        "    print(f\"  F1 Scores  - Humor: {f1_humor:.4f}, Sarcasm: {f1_sarcasm:.4f}, Offense: {f1_offense:.4f}, Motivation: {f1_motivation:.4f}, Avg: {avg_f1:.4f}\")\n",
        "    print(f\"  Accuracy   - Humor: {acc_humor:.4f}, Sarcasm: {acc_sarcasm:.4f}, Offense: {acc_offense:.4f}, Motivation: {acc_motivation:.4f}, Avg: {avg_acc:.4f}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}